{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Reduction with PCA\n",
    "\n",
    "In this notebook, we will take a look at how PCA can be used to visualize high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset\n",
    "\n",
    "To use some realistic data, let's load the famous MNIST dataset, consisting of hand drawn digits $0,1,\\ldots,9$. There is a built in `sklearn` function to load this dataset (it loads a small version of the full MNIST dataset, with a relatively small number of low-resolution images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version of MNIST contains 1797 samples of handwritten digits, represented $8 \\times 8$ images. We can think of these as vectors in $\\mathbb{R}^{8 \\times 8} \\approx \\mathbb{R}^{64}$.\n",
    "\n",
    "Here are a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(digits.images[j], cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get nicer-looking pictures by applying a filter to smooth the data (this is only for visualization purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "\n",
    "for j in range(10):\n",
    "    fig.add_subplot(2,5,j+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(digits.images[j], cmap='gray', interpolation = 'lanczos')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA on MNIST\n",
    "\n",
    "Recall that our `digits.data` matrix has shape 1797 x 64 = samples x features. \n",
    "\n",
    "Using our terminology from class, we have a dataset $$\\{\\vec{x}_1,\\ldots,\\vec{x}_{1797}\\},$$ where each $\\vec{x}_i \\in \\mathbb{R}^{64}$. This is recorded as the \"data matrix\" \n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "\\vec{x}_1^T \\\\\n",
    "\\vec{x}_2^T \\\\\n",
    "\\cdots \\\\\n",
    "\\vec{x}_{1797}^T \\end{pmatrix},\n",
    "$$\n",
    "where the superscript $T$ indicatest the *transpose* operation, turning a column vector into a row vector.\n",
    "\n",
    "We can apply PCA to this data matrix. Recall from class that PCA returns an orthonormal basis $\\{\\vec{v}_1,\\ldots,\\vec{v}_{64}\\}$, defined as follows. The first principal vector $\\vec{v}_1$ solves the optimization problem \n",
    "$$\n",
    "\\mathrm{minimize} \\quad \\sum_{i=1}^{1797} \\left\\| \\vec{x}_i - \\langle \\vec{x}_i, \\vec{v} \\rangle \\vec{v}\\right\\|^2 \\quad \\mbox{among all unit vectors $\\vec{v}$}.\n",
    "$$\n",
    "The second principal vector solves the optimization problem\n",
    "$$\n",
    "\\mathrm{minimize} \\quad \\sum_{i=1}^{1797} \\left\\| \\vec{x}_i - \\langle \\vec{x}_i, \\vec{v} \\rangle \\vec{v}\\right\\|^2 \\quad \\mbox{among all unit vectors $\\vec{v}$ which are orthogonal to $\\vec{v}_1$}.\n",
    "$$\n",
    "The remaining vectors in the basis are defined recursively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We haven't learned how to actually solve this problem yet, but we can use a built-in method from `sklearn` to do it computationally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA() # Initialize the model\n",
    "pca.fit(digits.data) # fit the model to our data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted PCA model has various attributes. The singularvectors we are interested are stored in the `components_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.components_.shape)\n",
    "print(pca.components_[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, `pca.components_` is a 64x64 matrix containing all PCA vectors of our data matrix as its columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, the motivation for PCA was to find a good low-dimensional subspace to project our data onto.\n",
    "\n",
    "Given the PCA basis $\\{\\vec{v}_1,\\ldots,\\vec{v}_{64}\\}$, we can project a vector $\\vec{x} \\in \\mathbb{R}^{64}$ onto the 2-dimensional subspace $\\mathrm{span}\\left[\\{\\vec{v}_1,\\vec{v}_2\\}\\right] \\subset \\mathbb{R}^{64}$ via the map\n",
    "$$\n",
    "\\vec{x} \\mapsto \\left[\\begin{matrix} \\langle \\vec{x},\\vec{v}_1 \\rangle \\\\ \\langle \\vec{x},\\vec{v}_2 \\rangle \\end{matrix}\\right].\n",
    "$$\n",
    "This simple formula is due to the fact that the PCA basis is orthonormal!\n",
    "\n",
    "Since this subspace is 2-dimensional, we can visualize the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pVec0 = pca.components_[0]\n",
    "pVec1 = pca.components_[1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "projectedMNIST = []\n",
    "\n",
    "# Now we linearly project every sample in our dataset onto the two-dimensional subspace spanned by the first \n",
    "# two singular vectors\n",
    "for j in range(1797):\n",
    "    projectedMNIST.append([np.dot(digits.data[j],pVec0),np.dot(digits.data[j],pVec1)])\n",
    "\n",
    "projectedMNIST = np.array(projectedMNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we projected onto a two-dimensional plane, we can now visualize our data!\n",
    "\n",
    "The figure below shows our 64-dimensional data, projected onto our 2-dimensional PCA subspace. The dots are colored by their 'class' (i.e., which digit is drawn, 0,1,...,9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(projectedMNIST[:, 0], projectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('pVec0')\n",
    "plt.ylabel('pVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our data is separated out in some sensible way according to class!\n",
    "\n",
    "As a **baseline**, here's what we get if we project onto a random 2-dimensional subspace of our 64-dimensional data space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orth\n",
    "\n",
    "RandVecs = orth(np.random.rand(64,2))\n",
    "\n",
    "RandVec0 = RandVecs[:,0]\n",
    "RandVec1 = RandVecs[:,1]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "RandProjectedMNIST = []\n",
    "\n",
    "for j in range(1797):\n",
    "    RandProjectedMNIST.append([np.dot(digits.data[j],RandVec0),np.dot(digits.data[j],RandVec1)])\n",
    "\n",
    "RandProjectedMNIST = np.array(RandProjectedMNIST)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.scatter(RandProjectedMNIST[:, 0], RandProjectedMNIST[:, 1],\n",
    "            c=digits.target, alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('nipy_spectral_r', 10))\n",
    "\n",
    "plt.xlabel('RandVec0')\n",
    "plt.ylabel('RandVec1')\n",
    "plt.axis('equal')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that PCA is really doing some work for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this image application, we can also visualize our principal vectors--we just need to reshape the 64-dimensional vectors into 8x8 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(pVec0.reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('First Principal Vector')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(pVec1.reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('Second Principal Vector')\n",
    "\n",
    "k = 60\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pca.components_[k].reshape(8,8),cmap='gray', interpolation = 'sinc')\n",
    "plt.axis('off')\n",
    "plt.title('Principal Vector '+str(k+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eyeballing (so we are perhaps subject to confirmation bias...), we see that the first principal vector seems sensitive to picking out shapes that look like '4', while the second is sensitive to shapes that look like '0'. This is confirmed by the scatter plot. \n",
    "\n",
    "Moreover, looking at the negative PC1 direction, it seems that '3' is the \"least '4'-like\" digit. The PC2 direction shows that '1' is the \"least '0'-like\" digit. \n",
    "\n",
    "As we increas $k$, the $k$th principal vector becomes less and less interpretable to human eyes.\n",
    "\n",
    "Typically, the first few principal vectors capture the majority of the variability in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement above can be quantified using the *explained variance ratio*, which is the function\n",
    "\\begin{align*}\n",
    "\\mathrm{EVR}:\\{1,\\ldots,m\\} &\\to \\mathbb{R} \\\\\n",
    "k &\\mapsto \\frac{1}{T} \\sum_{i=1}^k \\sigma_i^2,\n",
    "\\end{align*}\n",
    "where $\\sigma_i$ is the $i$-th singular value of the data matrix, $m$ is the number of samples, and $T$ is the total sum of squared singular values (to normalize).\n",
    "\n",
    "**Note:** We haven't officially defined singular values yet---that will be done this week---but the plots below are still intuitive.\n",
    "\n",
    "\n",
    "For our data, this looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.title('Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the first few principal vectors are really doing the heavy lifting in representing the data.\n",
    "\n",
    "It is also informative to look at the 'running total' of explained variance, which shows how much total variance is explained as a percentage by the first $k$ principal vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that 80% of the variation in the data is captured by ~10 principal vectors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
