{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explore further applications of eigenvalues and eigenvectors; namely:\n",
    "- Spectral Clustering\n",
    "- Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "\n",
    "We'll start with *spectral clustering*. This is an unsupervised learning algorithm for determining \"clusters\" in a dataset. We'll start with finding clusters or communities in a graph. Let's import some packages --- notice that we are using a package we haven't used before. The package `networkx` is useful for manipulating graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx #For graph analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a graph object in `networkx` to experiment with. We'll purposefully create a graph with some apparent communities. The package contains several graph generator functions---we'll start with a `connected_caveman_graph`, which is a highly regular graph with community structure, then drop some nodes and add edges to \"randomize\" it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.connected_caveman_graph(3,8) # Create an initial community-structured graph\n",
    "G.remove_nodes_from([1,3,20]) # Drop some random nodes\n",
    "G.add_edges_from([(2,8),(4,9),(12,18)]) # Add some Random Edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many methods for visualizing graphs---i.e., drawing nodes in the plane and line segments connecting them when an edge is present. I like the 'Kamada-Kawai' layout, which places the nodes using a 'force-directed' algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the picture, we intuitively see 'communities' in the graph. Observe that the communities here are not as strict as the *connected components* that we discussed in the last class. This brings us to our \n",
    "\n",
    "**Goal:** Find an algorithm to discover the communities in a given graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices Associated to a Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from class that there are several matrices we can associate to a graph. We'll use `networkx` to generate them for our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the **adjacency matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = nx.to_numpy_array(G)\n",
    "print(f'The number of nodes of G is {len(G)}. The size of A is {A.shape[0]} by {A.shape[1]}')\n",
    "print()\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For larger matrices, it is frequently not so useful to look at the numbers printed out. It can be more informative to look at the matrix as an image, with entries colored according to their numerical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have the **incidence matrix**. \n",
    "\n",
    "Note: the `incidence_matrix` function returns an object which is of 'sparse matrix' data type. We use the attribute `.todense` to turn it into a normal `numpy` array. \n",
    "\n",
    "Also, the matrix returned by the function is the transpose of how we defined it in class. To fix this, we use `.T` to take a transpose. This is not necessary, but is only being done for consistency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.incidence_matrix(G, oriented = True).todense().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of nodes of G is {len(G)} and the number of edges of G is {len(G.edges())}. The size of B is {B.shape[0]} by {B.shape[1]}')\n",
    "print()\n",
    "plt.imshow(B)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the **graph Laplacian**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nx.laplacian_matrix(G).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The number of nodes of G is {len(G)}. The size of L is {L.shape[0]} by {L.shape[1]}')\n",
    "print()\n",
    "plt.imshow(L)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from class that we had two formulas for $L$. Either\n",
    "$$\n",
    "L = B^T B\n",
    "$$\n",
    "or \n",
    "$$\n",
    "L = D - A,\n",
    "$$\n",
    "where $D$ is the **degree matrix** for $G$. Let's test and make sure that our formulas agree with the output of the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First formula\n",
    "L_test_1 = B.T@B\n",
    "\n",
    "print('The Laplacian, using the first formula:')\n",
    "print()\n",
    "plt.imshow(L_test_1)\n",
    "plt.show()\n",
    "\n",
    "worst_error = np.max(np.abs(L - L_test_1))\n",
    "print(f'The largest entry of L - B^T B is {worst_error}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second formula\n",
    "D = np.diag([G.degree(i) for i in G.nodes()])\n",
    "print('The degree matrix:')\n",
    "print()\n",
    "plt.imshow(D)\n",
    "plt.show()\n",
    "\n",
    "L_test_2 = D - A\n",
    "\n",
    "print('The Laplacian, using the second formula:')\n",
    "print()\n",
    "plt.imshow(L_test_2)\n",
    "plt.show()\n",
    "\n",
    "worst_error = np.max(np.abs(L - L_test_2))\n",
    "print(f'The largest entry of L - B^T B is {worst_error}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectrum of the Laplacian\n",
    "\n",
    "Now we will look at the *spectrum* of the Laplacian --- i.e., its sorted list of eigenvalues --- and show how it relates to community detection in the graph.\n",
    "\n",
    "The following code computes the eigenvalues and eigenvectors of the Laplacian. The eigenvalues are sorted in *increasing* order, and eigenvectors are sorted the same way (i.e. the first column of `vects` corresponds to the first entry of `vals`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, vects = np.linalg.eigh(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The eigenvalues of L:')\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that we have exactly one eigenvalue which is equal to zero (up to numerical error). This agrees with the theory that we developed last time: the number of zero eigenvalues correspons to the number of path components in the graph, and there is one path component in our example.\n",
    "\n",
    "However, there are a few which are 'near' zero. Let's plot the eigenvalues to see the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vals,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that there are three eigenvalues 'near' zero, then a big jump. This corresponds to the fact that there are three communities in our example graph. This jump is what we look for when doing community detection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection: Hierarchical Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors of $L$ also tell us about the community structure.\n",
    "\n",
    "For a graph with one path component, the first eigenvector will always be constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second eigenvector is called the *Fiedler vector* for the graph. It tells us how to split the graph into two communities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vects[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing to notice is the split between positive and negative entries. We can use this split to color the nodes of our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(np.array(np.abs(np.ceil(vects[:,1])).T).flatten())\n",
    "\n",
    "nx.draw_kamada_kawai(G,node_color = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we suspect that there are three communities, based on eigenvalue information, we could keep the larger community in this picture and run the same process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However** let's forgo this approach and use a more common one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Detection: Embedding Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we use the eigenvectors to embed the nodes of our graph into some Euclidean space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the second and third columns of our eigenvector matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vects[:,1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to treat this as a 'data matrix'. Each row is a sample and each column is a feature. This turns the graph into a dataset of vectors, which is much easier to handle than graph data! Let's plot our dataset as a scatterplot in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.array(vects[:,1].T).flatten()\n",
    "ys = np.array(vects[:,2].T).flatten()\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are three apparent clusters in this vector dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add another column (the next eigenvector) and get a dataset in 3D, which also has three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D # For 3D plotting\n",
    "\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "xs = vects[:,1]\n",
    "ys = vects[:,2]\n",
    "zs = vects[:,3]\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many standard algorithms for clustering datasets in a vector space --- we haven't seen these in class yet, but we will later in the semester. These can now be applied to the vector space data that we've engineered from the graph. This can then be translated into community detection in the original graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's use $k$-Means Clustering on the 3D embedded data, with $k=3$ (this is the target number clusters, which we can infer from the observation that $L$ has three small eigenvalues). We'll apply the `sklearn` version of $k$-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3)\n",
    "data = vects[:,1:4]\n",
    "kmeans.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering found by $k$-Means is encoded in the `labels_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these labels to color our graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(kmeans.labels_)\n",
    "\n",
    "nx.draw_kamada_kawai(G,node_color = colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it did a good job picking out communities!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Clustering for a Dataset of Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral Clustering can be applied even if we start with data that doesn't live on a graph. Consider the following 'toy' dataset from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "X,y = make_circles(n_samples = 200, noise = 0.05,factor = 0.5)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually, there seem to be two 'clusters' in this dataset. Since we are starting with data in a vector space (not a graph), we could apply $k$-Means clustering directly to try to obtain a clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our cluster labels to color the data, we see that it did a bad job. This is due to the nature of the $k$-Means algorithm, which is easily confused by the 'nested' nature of the data in this picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0],X[:,1],c = list(kmeans.labels_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use spectral clustering to do a better job, but first we need a graph!\n",
    "\n",
    "On option is to construct a $k$-neighbors graph (this is a *different* $k$ than the one in $k$-Means!). After choosing a $k$, we connect data point $x$ to datapoint $y$ if $y$ is one of the $k$ closest points to $x$ in the dataset. Code for constructing the graph and turning it into a `networkx` graph object is given below.\n",
    "\n",
    "Let's take $k = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "A = kneighbors_graph(X, n_neighbors=5).todense() #kneighbors_graph returns a sparse matrix\n",
    "\n",
    "G = nx.from_numpy_array(A)\n",
    "\n",
    "nx.draw_kamada_kawai(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run through the process of spectral clustering described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nx.laplacian_matrix(G).todense()\n",
    "vals, vects = np.linalg.eigh(L)\n",
    "\n",
    "plt.plot(vals[:10],'o')\n",
    "plt.title('First Ten Laplacian eigenvalues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we have two zero eigenvalues, indicating that the graph actually has two path components. Let's run the spectral embedding using the first 3 eigenvectors, then do $k$-means clustering with $k=2$. \n",
    "\n",
    "We will then plot our original data with coloring determined by the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2)\n",
    "kmeans.fit(vects[:,:3])\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c = list(kmeans.labels_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the clustering agrees with our intuition!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish off this section, we should mention that the whole spectral clustering pipeline is implemented as an `sklearn` model. Here is the code for running the above example using `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "SC = SpectralClustering(n_clusters = 2, affinity = 'nearest_neighbors')\n",
    "SC.fit(X)\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c = list(SC.labels_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in class, an arbitrary matrix $A \\in \\mathbb{R}^{m \\times n}$ (with, say, $m \\geq n$) can always be decomposed as \n",
    "$$\n",
    "A = U \\Sigma V^T,\n",
    "$$\n",
    "where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are *orthogonal matrices* (their columns for an orthonormal basis for their respective vector spaces) and $\\Sigma$ has a block diagonal structure.\n",
    "\n",
    "In particular, the columns of $V$ are eigenvectors of $A^T A$ and one block of $\\Sigma$ contains square roots of the (real, non-negative) eigenvalues of $A^T A$.  \n",
    "\n",
    "We will now show how to use the SVD for matching point clouds in a geometric data analysis context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Processing Data\n",
    "\n",
    "We'll use a dataset consisting of point clouds sampled from 3D models of horse figures. The models in the dataset come from an animation of a horse running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='./data/meshes.npy'\n",
    "tab_obj=np.load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "X=tab_obj[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point Cloud Registration\n",
    "\n",
    "In this section, we aim to address the the\n",
    "\n",
    "**Point Cloud Registration Problem:** Given two 3D point cloud models, how do we optimally align the point clouds in space, as a processing step for downstream analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that we're using is already aligned pretty well, so let's write a function to introduce adversarial rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomlyRotatePointCloud(X, angle_bound = np.pi/6):\n",
    "    \n",
    "    theta1 = np.random.rand()*angle_bound\n",
    "    A1 = np.array([[1,0,0],\n",
    "                  [0, np.cos(theta1),-np.sin(theta1)],\n",
    "                   [0, np.sin(theta1),np.cos(theta1)]])\n",
    "    theta2 = np.random.rand()*angle_bound\n",
    "    A2 = np.array([[np.cos(theta2),0,-np.sin(theta2)],\n",
    "                  [0, 1, 0],\n",
    "                   [np.sin(theta2), 0,np.cos(theta2)]])\n",
    "    theta3 = np.random.rand()*angle_bound\n",
    "    A3 = np.array([[np.cos(theta3),-np.sin(theta3),0],\n",
    "                  [np.sin(theta3),np.cos(theta3) ,0],\n",
    "                   [0,0 ,1]])\n",
    "    \n",
    "    X_rot = X@A3@A2@A1\n",
    "    \n",
    "    return X_rot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = randomlyRotatePointCloud(X)\n",
    "\n",
    "# The following centers the pointclouds to help with visualization\n",
    "X -= np.mean(X,axis = 0)\n",
    "Y -= np.mean(Y,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,10))\n",
    "\n",
    "ax = fig.add_subplot(121, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()\n",
    "\n",
    "ax = fig.add_subplot(122, projection='3d')\n",
    "ax.scatter(Y[:,0],Y[:,2],Y[:,1], marker='o', s=20, c='blue', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the point clouds on top of one another shows the misalignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.scatter(Y[:,0],Y[:,2],Y[:,1], marker='o', s=20, c='blue', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we do the alignment?\n",
    "\n",
    "**Answer:** Using Singular Value Decompositions (SVDs)!\n",
    "\n",
    "Here is the procedure:\n",
    "\n",
    "- Given two 3D point clouds with the same number points $n$ that are already in correspondence$^{**}$ consider them as matrices $X,Y \\in \\mathbb{R}^{n \\times 3}$.\n",
    "\n",
    "- Construct the matrix $A = X^TY \\in \\mathbb{R}^{n \\times n}$. \n",
    "\n",
    "- Compute the SVD $A = U \\Sigma V^T$. \n",
    "\n",
    "- The optimal realignment of $Y$ to $X$ is given by $YVU^T$. \n",
    "\n",
    "$**$ Here, we are making a **big assumption:** we assume that the $i$th of the matrix $X$ should really be in correspondence with the $i$th row of matrix $Y$---i.e., we don't have a point on the nose of one horse in row 1 of $X$, and a point on the tail of the other horse in row 1 of $Y$. Time permitting, we will discuss ways in which this assumption can be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X.T@Y\n",
    "U, Sigma, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "YRot = Y@Vt.T@U.T\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.scatter(YRot[:,0],YRot[:,2],YRot[:,1], marker='o', s=20, c='blue', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Example\n",
    "\n",
    "Let's try this when we don't have a copy of exactly the same horse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "idy = 2\n",
    "\n",
    "X=tab_obj[idx]\n",
    "Y=tab_obj[idy]\n",
    "\n",
    "Y = randomlyRotatePointCloud(Y)\n",
    "\n",
    "# The following centers the pointclouds to help with visualization\n",
    "X -= np.mean(X,axis = 0)\n",
    "Y -= np.mean(Y,axis = 0)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.scatter(Y[:,0],Y[:,2],Y[:,1], marker='o', s=20, c='blue', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X.T@Y\n",
    "U, Sigma, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "\n",
    "YRot = Y@Vt.T@U.T\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:,0],X[:,2],X[:,1], marker='o', s=20, c='goldenrod', alpha=0.6)\n",
    "ax.scatter(YRot[:,0],YRot[:,2],YRot[:,1], marker='o', s=20, c='blue', alpha=0.6)\n",
    "ax.view_init(elev=10., azim=360)\n",
    "ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
